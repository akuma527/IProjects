{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_Model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87f097d408b74030b083bc5a3e52ed9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aa303c50c18d4b73955e45c8b6bbf748",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc4739a3e50c497893f246c370bfef29",
              "IPY_MODEL_8f67a96ce2bb42d2aa1fe666ed45295c"
            ]
          }
        },
        "aa303c50c18d4b73955e45c8b6bbf748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc4739a3e50c497893f246c370bfef29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1653071c6704a0c838f151bd157f6df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f5ce262216547b0be2dbdf40e5c3fe4"
          }
        },
        "8f67a96ce2bb42d2aa1fe666ed45295c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33d46ec66d3a4da58971017cb0be11fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 30/30 [24:49&lt;00:00, 49.72s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53066919ecd8431c9c73965b807079ab"
          }
        },
        "d1653071c6704a0c838f151bd157f6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f5ce262216547b0be2dbdf40e5c3fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33d46ec66d3a4da58971017cb0be11fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53066919ecd8431c9c73965b807079ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T07:43:01.901490Z",
          "start_time": "2020-03-14T07:42:23.760181Z"
        },
        "colab_type": "code",
        "id": "_AFSb318cKKG",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import numpy as np\n",
        "import random\n",
        "import tarfile\n",
        "import io\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPGfyzqasHOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "1e6479eb-6f56-4a22-91e8-1afbf7728c01"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owSngYsxsTzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc6908ee-14a1-4f82-cbd3-3bbd79b4182b"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/AutoTagImages/dataset/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/AutoTagImages/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T07:43:29.527471Z",
          "start_time": "2020-03-14T07:43:29.514176Z"
        },
        "colab_type": "code",
        "id": "yVVAHOAvjpq0",
        "outputId": "7c4300e7-6f81-4b28-a12e-f8165d92f604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "os.listdir('train_images/')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T07:43:50.179223Z",
          "start_time": "2020-03-14T07:43:49.969155Z"
        },
        "colab_type": "code",
        "id": "z0yQGgxZEclJ",
        "outputId": "00773f74-95be-47f3-f8df-8d7be2493477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "la = LabelEncoder()\n",
        "la.fit(pd.read_csv('train.csv')['Class'])"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:08.546172Z",
          "start_time": "2020-03-14T08:48:08.531210Z"
        },
        "colab_type": "code",
        "id": "8sLNKc05j3YV",
        "colab": {}
      },
      "source": [
        "class YourDataset(Dataset):\n",
        "    def __init__(self, txt_path='filelist.txt', img_dir='data', testdata=False, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize data set as a list of IDs corresponding to each item of data set\n",
        "\n",
        "        :param img_dir: path to image files as a uncompressed tar archive\n",
        "        :param txt_path: a text file containing names of all of images line by line\n",
        "        :param transform: apply some transforms like cropping, rotating, etc on input image\n",
        "        \"\"\"\n",
        "\n",
        "        df = pd.read_csv(txt_path)\n",
        "        self.img_names = df.Image.values\n",
        "        self.testdata = testdata\n",
        "        if not self.testdata:\n",
        "            self.target =(la.transform(df['Class']).reshape(-1,1))\n",
        "        self.txt_path = txt_path\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.to_tensor = ToTensor()\n",
        "        self.to_pil = ToPILImage()\n",
        "        self.get_image_selector = True if img_dir.__contains__('tar') else False\n",
        "        self.tf = tarfile.open(self.img_dir) if self.get_image_selector else None\n",
        "\n",
        "    def get_image_from_tar(self, name):\n",
        "        \"\"\"\n",
        "        Gets a image by a name gathered from file list csv file\n",
        "\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "        image = self.tf.extractfile(name)\n",
        "        image = image.read()\n",
        "        image = Image.open(io.BytesIO(image))\n",
        "        return image\n",
        "\n",
        "    def get_image_from_folder(self, name):\n",
        "        \"\"\"\n",
        "        gets a image by a name gathered from file list text file\n",
        "\n",
        "        :param name: name of targeted image\n",
        "        :return: a PIL image\n",
        "        \"\"\"\n",
        "        image = cv2.imread(os.path.join(self.img_dir, name))\n",
        "        # print(self.img_dir, name)\n",
        "        # image = Image.open(os.path.join(self.img_dir, name))\n",
        "        myimage = Image.fromarray(image)\n",
        "        return myimage\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of data set using list of IDs\n",
        "\n",
        "        :return: number of samples in data set\n",
        "        \"\"\"\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one item of data set.\n",
        "\n",
        "        :param index: index of item in IDs list\n",
        "\n",
        "        :return: a sample of data as a dict\n",
        "        \"\"\"\n",
        "\n",
        "        if index == (self.__len__() - 1) and self.get_image_selector:  # close tarfile opened in __init__\n",
        "            self.tf.close()\n",
        "\n",
        "        if self.get_image_selector:  # note: we prefer to extract then process!\n",
        "            X = self.get_image_from_tar(self.img_names[index])\n",
        "        else:\n",
        "            X = self.get_image_from_folder(self.img_names[index])\n",
        "            \n",
        "            if not self.testdata:\n",
        "                Y = self.target[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        if not self.testdata:\n",
        "            sample = {'X': X,\n",
        "                      'Y': Y}\n",
        "        if self.testdata:\n",
        "            sample = {'X': X}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M9whwwyWcKKi"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:09.424627Z",
          "start_time": "2020-03-14T08:48:09.420632Z"
        },
        "colab_type": "code",
        "id": "AybH6S6AcKKk",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:09.691479Z",
          "start_time": "2020-03-14T08:48:09.687489Z"
        },
        "colab_type": "code",
        "id": "kXukkolEcKKo",
        "colab": {}
      },
      "source": [
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dF2JQTIrcKKr"
      },
      "source": [
        "# Defining Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:14.201026Z",
          "start_time": "2020-03-14T08:48:12.277172Z"
        },
        "colab_type": "code",
        "id": "OMUSa_rjcKKs",
        "colab": {}
      },
      "source": [
        "model = models.vgg11(pretrained=True, progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlLX8XTJ0v5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Identity(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Identity, self).__init__()\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         return x\n",
        "\n",
        "# model.classifier = Identity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:14.209007Z",
          "start_time": "2020-03-14T08:48:14.203022Z"
        },
        "colab_type": "code",
        "id": "5PugeIHfcKKv",
        "colab": {}
      },
      "source": [
        "# Freezing other layers of the model\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw-zWqi61era",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "e5b64778-f83f-4aad-d137-1c0cca454b54"
      },
      "source": [
        "model"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FON2KZ24brXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.classifier[6] = (nn.Linear(4096, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA7MLcIb_q_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.classifier = nn.Sequential(*list(model.classifier) + [nn.ReLU(inplace=True)] + [nn.Linear(10, 4)] + [nn.Softmax(dim=1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:15.781924Z",
          "start_time": "2020-03-14T08:48:15.775939Z"
        },
        "colab_type": "code",
        "id": "F-v8EdwNfRvO",
        "outputId": "15e97663-d38f-4f72-8465-eac5f05a067f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:17.287848Z",
          "start_time": "2020-03-14T08:48:16.491849Z"
        },
        "colab_type": "code",
        "id": "yhR83q_mfndm",
        "outputId": "782f9699-9281-454e-ccb7-f95b48cf5c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Linear(in_features=10, out_features=4, bias=True)\n",
              "    (9): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzjKMUtl3xON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "ea7976cc-8ec6-4d63-9a72-d28e8e6d251b"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
            "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-5        [-1, 128, 112, 112]               0\n",
            "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
            "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
            "              ReLU-8          [-1, 256, 56, 56]               0\n",
            "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-10          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-11          [-1, 256, 28, 28]               0\n",
            "           Conv2d-12          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-13          [-1, 512, 28, 28]               0\n",
            "           Conv2d-14          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-15          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
            "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-18          [-1, 512, 14, 14]               0\n",
            "           Conv2d-19          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-20          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-21            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0\n",
            "           Linear-23                 [-1, 4096]     102,764,544\n",
            "             ReLU-24                 [-1, 4096]               0\n",
            "          Dropout-25                 [-1, 4096]               0\n",
            "           Linear-26                 [-1, 4096]      16,781,312\n",
            "             ReLU-27                 [-1, 4096]               0\n",
            "          Dropout-28                 [-1, 4096]               0\n",
            "           Linear-29                   [-1, 10]          40,970\n",
            "             ReLU-30                   [-1, 10]               0\n",
            "           Linear-31                    [-1, 4]              44\n",
            "          Softmax-32                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 128,807,350\n",
            "Trainable params: 41,014\n",
            "Non-trainable params: 128,766,336\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 125.37\n",
            "Params size (MB): 491.36\n",
            "Estimated Total Size (MB): 617.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:19.678587Z",
          "start_time": "2020-03-14T08:48:19.672601Z"
        },
        "colab_type": "code",
        "id": "euW3scSJcKK9",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "# optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)#lr 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VSxh9hqycKLB"
      },
      "source": [
        "# Defining Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:20.816167Z",
          "start_time": "2020-03-14T08:48:20.807223Z"
        },
        "colab_type": "code",
        "id": "uDldrcjRcKLD",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define train transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5],  # 3 channels (RGB) for colored images\n",
        "                         [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define test transforms\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5],  # 3 channels (RGB) for colored images\n",
        "                         [0.5, 0.5, 0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:21.784374Z",
          "start_time": "2020-03-14T08:48:21.761445Z"
        },
        "colab_type": "code",
        "id": "-4-3qBP1cKLH",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "train_data = YourDataset(txt_path='train.csv', img_dir='train_images/1/', transform=train_transforms)\n",
        "test_data = YourDataset(txt_path='test.csv', img_dir='test_images/1/', testdata=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:22.831054Z",
          "start_time": "2020-03-14T08:48:22.825069Z"
        },
        "colab_type": "code",
        "id": "0dzjLDCncKLK",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# calculate size of train and validation sets\n",
        "train_size = int(0.8 * len(train_data))\n",
        "valid_size = len(train_data) - train_size\n",
        "partial_train_ds, valid_ds = random_split(train_data, [train_size, valid_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:23.351597Z",
          "start_time": "2020-03-14T08:48:23.345607Z"
        },
        "colab_type": "code",
        "id": "D1yF61dBcKLO",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# replace XX and YY with batch_size and number of workers, respectively\n",
        "train_loader = DataLoader(partial_train_ds, batch_size=128, num_workers=8)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=128, num_workers=8)\n",
        "test_loader = DataLoader(test_data, batch_size=128, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T08:48:24.993428Z",
          "start_time": "2020-03-14T08:48:24.989438Z"
        },
        "colab_type": "code",
        "id": "UK3ZIyHmKS6l",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-14T11:30:20.113680Z",
          "start_time": "2020-03-14T08:54:39.257061Z"
        },
        "colab_type": "code",
        "id": "_Wzh02G3K6Nc",
        "outputId": "6172a3a2-ab6a-44d4-e51b-27f0f08a7c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "87f097d408b74030b083bc5a3e52ed9e",
            "aa303c50c18d4b73955e45c8b6bbf748",
            "cc4739a3e50c497893f246c370bfef29",
            "8f67a96ce2bb42d2aa1fe666ed45295c",
            "d1653071c6704a0c838f151bd157f6df",
            "9f5ce262216547b0be2dbdf40e5c3fe4",
            "33d46ec66d3a4da58971017cb0be11fb",
            "53066919ecd8431c9c73965b807079ab"
          ]
        }
      },
      "source": [
        "n_epochs = 30 # this is a hyperparameter you'll need to define\n",
        "k = []\n",
        "for epoch in tqdm_notebook(range(n_epochs)):\n",
        "    ##################\n",
        "    ### TRAIN LOOP ###\n",
        "    ##################\n",
        "    # set the model to train mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i in (train_loader.dataset):\n",
        "        # clear the old gradients from optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: feed inputs to the model to get outputs\n",
        "        data, target = i['X'], i['Y']\n",
        "        # target = torch.autograd.Variable(torch.tensor(target))\n",
        "        data = data.view(-1, 3, 224, 224).to(device)\n",
        "        target = Variable(torch.tensor(target, dtype=torch.long)).to(device)\n",
        "        output = model(data)\n",
        "        \n",
        "        predicted = torch.argmax(output,1)\n",
        "        # calculate the training batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward: perform gradient descent of the loss w.r. to the model params\n",
        "        loss.backward()\n",
        "        # update the model parameters by performing a single optimization step\n",
        "        optimizer.step()\n",
        "        # accumulate the training loss\n",
        "        total += target.size(0)\n",
        "        # print(predicted, target)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        # k+=1\n",
        "        # print(k)\n",
        "\n",
        "    #######################\n",
        "    ### VALIDATION LOOP ###\n",
        "    #######################\n",
        "    # set the model to eval mode\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    # turn off gradients for validation\n",
        "    with torch.no_grad():\n",
        "        for i in (valid_loader.dataset):\n",
        "            # forward pass\n",
        "            data, target = i['X'], i['Y']\n",
        "            data = data.view(-1, 3, 224, 224).to(device)\n",
        "            target = Variable(torch.tensor(target, dtype=torch.long)).to(device)\n",
        "            output = model(data)\n",
        "            # validation batch loss\n",
        "            loss = criterion(output, target) \n",
        "            # accumulate the valid_loss\n",
        "            valid_loss += loss.item()\n",
        "            \n",
        "    #########################\n",
        "    ## PRINT EPOCH RESULTS ##\n",
        "    #########################\n",
        "    train_loss /= len(train_loader)\n",
        "    valid_loss /= len(valid_loader)\n",
        "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')\n",
        "    print(f'Accuracy of the network on the images Training accuracy: {100 * correct / total}')\n",
        "    "
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f097d408b74030b083bc5a3e52ed9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30.. Training loss: 155.98894472969207.. Validation Loss: 146.65244216918944\n",
            "Accuracy of the network on the images Training accuracy: 48.955286251567074\n",
            "Epoch: 2/30.. Training loss: 152.59964251831957.. Validation Loss: 142.67203028798104\n",
            "Accuracy of the network on the images Training accuracy: 51.65064772252403\n",
            "Epoch: 3/30.. Training loss: 151.99948292029532.. Validation Loss: 141.8154812037945\n",
            "Accuracy of the network on the images Training accuracy: 52.59089009611367\n",
            "Epoch: 4/30.. Training loss: 151.7589915535952.. Validation Loss: 139.9080340385437\n",
            "Accuracy of the network on the images Training accuracy: 52.56999582114501\n",
            "Epoch: 5/30.. Training loss: 150.2260099022012.. Validation Loss: 142.096968626976\n",
            "Accuracy of the network on the images Training accuracy: 53.802758044295864\n",
            "Epoch: 6/30.. Training loss: 148.74084336349839.. Validation Loss: 139.13357518911363\n",
            "Accuracy of the network on the images Training accuracy: 54.88926034266611\n",
            "Epoch: 7/30.. Training loss: 147.3430281454011.. Validation Loss: 139.02108405828477\n",
            "Accuracy of the network on the images Training accuracy: 55.89218554116172\n",
            "Epoch: 8/30.. Training loss: 148.5157443771237.. Validation Loss: 137.37943558096885\n",
            "Accuracy of the network on the images Training accuracy: 55.45340576681989\n",
            "Epoch: 9/30.. Training loss: 147.86689797671218.. Validation Loss: 138.27032333016396\n",
            "Accuracy of the network on the images Training accuracy: 55.996656916005016\n",
            "Epoch: 10/30.. Training loss: 147.1352693156192.. Validation Loss: 138.58963499069213\n",
            "Accuracy of the network on the images Training accuracy: 56.49811951525282\n",
            "Epoch: 11/30.. Training loss: 147.19023485246458.. Validation Loss: 138.80880928635597\n",
            "Accuracy of the network on the images Training accuracy: 56.08023401587965\n",
            "Epoch: 12/30.. Training loss: 146.4034463214247.. Validation Loss: 138.41524083614348\n",
            "Accuracy of the network on the images Training accuracy: 57.0622649394066\n",
            "Epoch: 13/30.. Training loss: 147.49077771996198.. Validation Loss: 135.18122114539148\n",
            "Accuracy of the network on the images Training accuracy: 56.10112829084831\n",
            "Epoch: 14/30.. Training loss: 146.23016927901068.. Validation Loss: 136.65969443321228\n",
            "Accuracy of the network on the images Training accuracy: 56.70706226493941\n",
            "Epoch: 15/30.. Training loss: 147.19255240026274.. Validation Loss: 136.64895556569098\n",
            "Accuracy of the network on the images Training accuracy: 56.60259089009612\n",
            "Epoch: 16/30.. Training loss: 145.43759909272194.. Validation Loss: 135.95695515871049\n",
            "Accuracy of the network on the images Training accuracy: 58.06519013790221\n",
            "Epoch: 17/30.. Training loss: 147.42824330612234.. Validation Loss: 139.4882412493229\n",
            "Accuracy of the network on the images Training accuracy: 55.9548683660677\n",
            "Epoch: 18/30.. Training loss: 147.30426637122505.. Validation Loss: 135.40416170358657\n",
            "Accuracy of the network on the images Training accuracy: 56.43543669034685\n",
            "Epoch: 19/30.. Training loss: 145.92800676508955.. Validation Loss: 136.49043201208116\n",
            "Accuracy of the network on the images Training accuracy: 57.54283326368575\n",
            "Epoch: 20/30.. Training loss: 147.46461639278814.. Validation Loss: 134.91188469529152\n",
            "Accuracy of the network on the images Training accuracy: 55.996656916005016\n",
            "Epoch: 21/30.. Training loss: 145.900583867964.. Validation Loss: 135.7277006685734\n",
            "Accuracy of the network on the images Training accuracy: 57.18763058921856\n",
            "Epoch: 22/30.. Training loss: 145.4769242029441.. Validation Loss: 139.6550816655159\n",
            "Accuracy of the network on the images Training accuracy: 58.00250731299624\n",
            "Epoch: 23/30.. Training loss: 147.17164544369044.. Validation Loss: 136.2466839849949\n",
            "Accuracy of the network on the images Training accuracy: 56.35185959047221\n",
            "Epoch: 24/30.. Training loss: 144.49130449796976.. Validation Loss: 134.9292475283146\n",
            "Accuracy of the network on the images Training accuracy: 58.46218136230673\n",
            "Epoch: 25/30.. Training loss: 145.52737107088691.. Validation Loss: 134.66628967523576\n",
            "Accuracy of the network on the images Training accuracy: 57.79356456330965\n",
            "Epoch: 26/30.. Training loss: 145.62133382339226.. Validation Loss: 136.0903391599655\n",
            "Accuracy of the network on the images Training accuracy: 57.730881738403674\n",
            "Epoch: 27/30.. Training loss: 145.80687525397852.. Validation Loss: 136.43050455451012\n",
            "Accuracy of the network on the images Training accuracy: 57.43836188884246\n",
            "Epoch: 28/30.. Training loss: 145.03222224116325.. Validation Loss: 136.19497764110565\n",
            "Accuracy of the network on the images Training accuracy: 58.69201838696197\n",
            "Epoch: 29/30.. Training loss: 145.62013606648696.. Validation Loss: 136.8139310359955\n",
            "Accuracy of the network on the images Training accuracy: 57.83535311324697\n",
            "Epoch: 30/30.. Training loss: 145.25036292327079.. Validation Loss: 135.99577957987785\n",
            "Accuracy of the network on the images Training accuracy: 58.06519013790221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgjhBPdKbFCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}